{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pythonstudy/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my 1st OOF...never give up\n"
     ]
    }
   ],
   "source": [
    "print(\"my 1st OOF...never give up\")\n",
    "SEED = 7    # CR7 always..\n",
    "path = 'Data/'\n",
    "\n",
    "application_train = pd.read_csv(path + \"application_train.csv\")\n",
    "application_test = pd.read_csv(path + \"application_test.csv\")\n",
    "POS_CASH = pd.read_csv(path + 'POS_CASH_balance.csv')\n",
    "credit_card = pd.read_csv(path + 'credit_card_balance.csv')\n",
    "bureau = pd.read_csv(path + 'bureau.csv')\n",
    "bureau_balance = pd.read_csv(path + 'bureau_balance.csv')\n",
    "previous_app = pd.read_csv(path + 'previous_application.csv')\n",
    "installments = pd.read_csv(path + 'installments_payments.csv')\n",
    "subm = pd.read_csv(path+ \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting...\n"
     ]
    }
   ],
   "source": [
    "#POS_CASH = pd.read_csv(path+ 'POS_CASH_balance.csv')\n",
    "print(\"Converting...\")\n",
    "le = LabelEncoder()\n",
    "POS_CASH['NAME_CONTRACT_STATUS'] = le.fit_transform(POS_CASH['NAME_CONTRACT_STATUS'].astype(str))\n",
    "nunique_status = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\n",
    "count_status = POS_CASH.groupby('SK_ID_CURR').size()\n",
    "#print(count_status)\n",
    "#print(nunique_status)\n",
    "POS_CASH['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\n",
    "POS_CASH['pc_COUNT_STATUS'] = count_status\n",
    "POS_CASH.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)\n",
    "#print(POS_CASH[POS_CASH['NUNIQUE_STATUS'].notna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit_card = pd.read_csv(path+ 'credit_card_balance.csv')\n",
    "\n",
    "credit_card['NAME_CONTRACT_STATUS'] = le.fit_transform(credit_card['NAME_CONTRACT_STATUS'].astype(str))\n",
    "nunique_status = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\n",
    "count_status = credit_card.groupby('SK_ID_CURR').size()\n",
    "credit_card['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\n",
    "credit_card['cc_COUNT_STATUS'] = count_status\n",
    "credit_card.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance = pd.read_csv(path + 'bureau_balance.csv')\n",
    "bureau_balance['STATUS'] = le.fit_transform(bureau_balance['STATUS'].astype(str))\n",
    "unique_status =bureau_balance[['SK_ID_BUREAU','STATUS']].groupby('SK_ID_BUREAU').nunique()\n",
    "count_status = bureau_balance.groupby('SK_ID_BUREAU').size()\n",
    "bureau_balance['NUNIQUE_STATUS'] = unique_status['STATUS']\n",
    "bureau_balance['bb_COUNT_STATUS'] = count_status\n",
    "bureau_balance.drop(['STATUS'], axis=1, inplace=True)\n",
    "\n",
    "bureau = pd.read_csv(path+ 'bureau.csv')\n",
    "bureau = bureau.merge(bureau_balance.groupby('SK_ID_BUREAU').mean().reset_index(), \n",
    "                                                      how = 'left', on= 'SK_ID_BUREAU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_status = bureau.groupby('SK_ID_CURR').size()\n",
    "bureau['B_COUNT_STATUS'] = count_status\n",
    "bureau_cat_features = [f for f in bureau.columns if bureau[f].dtype == 'object']\n",
    "for f in bureau_cat_features:\n",
    "    bureau[f] = le.fit_transform(bureau[f].astype(str))\n",
    "    nunique = bureau[['SK_ID_CURR', f]].groupby('SK_ID_CURR').nunique()\n",
    "    bureau['NUNIQUE_'+f] = nunique[f]\n",
    "    bureau.drop([f], axis=1, inplace=True)\n",
    "bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_status = previous_app.groupby('SK_ID_CURR').size()\n",
    "previous_app['pa_COUNT_STATUS'] = count_status\n",
    "\n",
    "previous_app_cat_features = [f for f in previous_app.columns if previous_app[f].dtype == 'object']\n",
    "for f in previous_app_cat_features:\n",
    "    previous_app[f] = le.fit_transform(previous_app[f].astype(str))\n",
    "    nunique = previous_app[['SK_ID_CURR', f]].groupby('SK_ID_CURR').nunique()\n",
    "    previous_app['NUNIQUE_'+f] = nunique[f]\n",
    "    previous_app.drop([f], axis=1, inplace=True)\n",
    "previous_app.drop(['SK_ID_PREV'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_status = installments.groupby('SK_ID_CURR').size()\n",
    "installments['ip_COUNT_STATUS'] = count_status\n",
    "\n",
    "installments.drop(['SK_ID_PREV'], axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging...\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging...\")\n",
    "data_train = application_train.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                             how='left', on='SK_ID_CURR')\n",
    "data_test = application_test.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                           how='left', on='SK_ID_CURR')\n",
    "\n",
    "data_train = data_train.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                         how='left', on='SK_ID_CURR')\n",
    "data_test = data_test.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                       how='left', on='SK_ID_CURR')\n",
    "                                                       \n",
    "data_train = data_train.merge(bureau.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                    how='left', on='SK_ID_CURR')\n",
    "data_test = data_test.merge(bureau.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                  how='left', on='SK_ID_CURR')\n",
    "                                                  \n",
    "data_train = data_train.merge(previous_app.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                          how='left', on='SK_ID_CURR')\n",
    "data_test = data_test.merge(previous_app.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                        how='left', on='SK_ID_CURR')\n",
    "\n",
    "data_train = data_train.merge(installments.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                         how = 'left', on='SK_ID_CURR')\n",
    "data_test =  data_test.merge(installments.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                        how = 'left', on='SK_ID_CURR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['X1'] = data_train['AMT_CREDIT_x']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X2'] = data_train['AMT_CREDIT_y']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X3'] = data_train['AMT_ANNUITY']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X4'] = data_train['AMT_ANNUITY_x']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X5'] = data_train['AMT_ANNUITY_y']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X6'] = data_train['AMT_CREDIT_x']/data_train['AMT_ANNUITY_x']\n",
    "data_train['X7'] = data_train['AMT_CREDIT_y']/data_train['AMT_ANNUITY_x']\n",
    "data_train['X8'] = data_train['AMT_CREDIT_x']/data_train['AMT_ANNUITY_y']\n",
    "data_train['X9'] = data_train['AMT_CREDIT_y']/data_train['AMT_ANNUITY_y']\n",
    "data_train['X10'] = data_train['AMT_CREDIT_SUM']/data_train['AMT_CREDIT_x']\n",
    "data_train['x11'] = data_train['AMT_CREDIT_SUM']/data_train['AMT_CREDIT_y']\n",
    "data_train['X12'] = data_train['AMT_CREDIT_SUM_DEBT']/data_train['AMT_CREDIT_x']\n",
    "data_train['X13'] = data_train['AMT_CREDIT_SUM_DEBT']/data_train['AMT_CREDIT_y']\n",
    "data_train['X14'] = data_train['AMT_CREDIT_SUM_DEBT']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X15'] = data_train['AMT_ANNUITY_x']/data_train['AMT_CREDIT_y']\n",
    "data_train['X16'] = data_train['AMT_ANNUITY_y']/data_train['AMT_CREDIT_x']\n",
    "data_train['X17'] = data_train['AMT_ANNUITY_x']/data_train['AMT_CREDIT_x']\n",
    "data_train['X18'] = data_train['AMT_ANNUITY_y']/data_train['AMT_CREDIT_y']\n",
    "\n",
    "\n",
    "data_test['X1'] = data_test['AMT_CREDIT_x']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X2'] = data_test['AMT_CREDIT_y']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X3'] = data_test['AMT_ANNUITY']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X4'] = data_test['AMT_ANNUITY_x']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X5'] = data_test['AMT_ANNUITY_y']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X6'] = data_test['AMT_CREDIT_x']/data_test['AMT_ANNUITY_x']\n",
    "data_test['X7'] = data_test['AMT_CREDIT_y']/data_test['AMT_ANNUITY_x']\n",
    "data_test['X8'] = data_test['AMT_CREDIT_x']/data_test['AMT_ANNUITY_y']\n",
    "data_test['X9'] = data_test['AMT_CREDIT_y']/data_test['AMT_ANNUITY_y']\n",
    "data_test['X10'] = data_test['AMT_CREDIT_SUM']/data_test['AMT_CREDIT_x']\n",
    "data_test['x11'] = data_test['AMT_CREDIT_SUM']/data_test['AMT_CREDIT_y']\n",
    "data_test['X12'] = data_test['AMT_CREDIT_SUM_DEBT']/data_test['AMT_CREDIT_x']\n",
    "data_test['X13'] = data_test['AMT_CREDIT_SUM_DEBT']/data_test['AMT_CREDIT_y']\n",
    "data_test['X14'] = data_test['AMT_CREDIT_SUM_DEBT']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X15'] = data_test['AMT_ANNUITY_x']/data_test['AMT_CREDIT_y']\n",
    "data_test['X16'] = data_test['AMT_ANNUITY_y']/data_test['AMT_CREDIT_x']\n",
    "data_test['X17'] = data_test['AMT_ANNUITY_x']/data_test['AMT_CREDIT_x']\n",
    "data_test['X18'] = data_test['AMT_ANNUITY_y']/data_test['AMT_CREDIT_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = data_train['TARGET']\n",
    "data_train.drop(['SK_ID_CURR', 'TARGET'], axis=1, inplace=True)\n",
    "data_test.drop(['SK_ID_CURR'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat features are: ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']\n",
      "[ 0  1  2  3  9 10 11 12 13 26 30 38 84 85 87 88]\n"
     ]
    }
   ],
   "source": [
    "cat_features = [f for f in data_train.columns if data_train[f].dtype == 'object']\n",
    "def column_index(df, query_cols):\n",
    "    cols = df.columns.values  ## col name을 array형태로 return 합니다.\n",
    "    sidx = np.argsort(cols)\n",
    "    return sidx[np.searchsorted(cols, query_cols, sorter=sidx)]\n",
    "cat_features_inds = column_index(data_train, cat_features)    \n",
    "print(\"Cat features are: %s\" % [f for f in cat_features])\n",
    "print(cat_features_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 229)\n"
     ]
    }
   ],
   "source": [
    "for col in cat_features:\n",
    "    data_train[col] = le.fit_transform(data_train[col].astype(str))\n",
    "    data_test[col] = le.fit_transform(data_test[col].astype(str))\n",
    "    \n",
    "data_train.fillna(-1, inplace=True)\n",
    "data_test.fillna(-1, inplace=True)\n",
    "cols = data_train.columns\n",
    "\n",
    "ntrain = data_train.shape[0]\n",
    "ntest = data_test.shape[0]\n",
    "\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(data_train.shape[0], n_folds=5, shuffle=True, random_state=7)\n",
    "NFOLDS = 5\n",
    "x_train = np.array(data_train)\n",
    "x_test = np.array(data_test)\n",
    "y_train = target_train.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lightgbmWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.seed = seed\n",
    "        \n",
    "    def train(self, x_tr, y_tr):\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_tr, y_tr, random_state=self.seed)\n",
    "        lgb_train = lgb.Dataset(data=x_train, label=y_train)\n",
    "        lgb_eval = lgb.Dataset(data=x_val, label=y_val)\n",
    "        print(\"Training..\")\n",
    "        self.lbgm = lgb.train(self.param, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=100, verbose_eval=10)\n",
    "\n",
    "    def predict(self, x):\n",
    "        print(\"Predicting..\")\n",
    "        return self.lbgm.predict(x)\n",
    "    \n",
    "\n",
    "def get_oof(clf):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)[:,1]  # or [:,0]\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)[:,1]  # or [:,0]\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "    \n",
    "def get_oof_xgb(clf):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)  # or [:,0]\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)  # or [:,0]\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {'task': 'train', \n",
    "          'boosting_type': 'gbdt', \n",
    "          'objective': 'binary', \n",
    "          'metric': 'auc', \n",
    "          'learning_rate': 0.01, \n",
    "          'num_leaves': 32, \n",
    "          'num_iteration': 500,\n",
    "          'verbose': 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lg..\n",
      "Training..\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.702035\n",
      "[20]\tvalid_0's auc: 0.708738\n",
      "[30]\tvalid_0's auc: 0.711159\n",
      "[40]\tvalid_0's auc: 0.713393\n",
      "[50]\tvalid_0's auc: 0.714894\n",
      "[60]\tvalid_0's auc: 0.71614\n",
      "[70]\tvalid_0's auc: 0.717316\n",
      "[80]\tvalid_0's auc: 0.718338\n",
      "[90]\tvalid_0's auc: 0.719157\n",
      "[100]\tvalid_0's auc: 0.71999\n",
      "[110]\tvalid_0's auc: 0.720987\n",
      "[120]\tvalid_0's auc: 0.72191\n",
      "[130]\tvalid_0's auc: 0.722622\n",
      "[140]\tvalid_0's auc: 0.723652\n",
      "[150]\tvalid_0's auc: 0.724688\n",
      "[160]\tvalid_0's auc: 0.725751\n",
      "[170]\tvalid_0's auc: 0.726681\n",
      "[180]\tvalid_0's auc: 0.727747\n",
      "[190]\tvalid_0's auc: 0.728957\n",
      "[200]\tvalid_0's auc: 0.730078\n",
      "[210]\tvalid_0's auc: 0.731331\n",
      "[220]\tvalid_0's auc: 0.732574\n",
      "[230]\tvalid_0's auc: 0.733867\n",
      "[240]\tvalid_0's auc: 0.735293\n",
      "[250]\tvalid_0's auc: 0.736472\n",
      "[260]\tvalid_0's auc: 0.737592\n",
      "[270]\tvalid_0's auc: 0.738618\n",
      "[280]\tvalid_0's auc: 0.739692\n",
      "[290]\tvalid_0's auc: 0.740777\n",
      "[300]\tvalid_0's auc: 0.742013\n",
      "[310]\tvalid_0's auc: 0.743245\n",
      "[320]\tvalid_0's auc: 0.744331\n",
      "[330]\tvalid_0's auc: 0.74544\n",
      "[340]\tvalid_0's auc: 0.746684\n",
      "[350]\tvalid_0's auc: 0.747946\n",
      "[360]\tvalid_0's auc: 0.749144\n",
      "[370]\tvalid_0's auc: 0.750352\n",
      "[380]\tvalid_0's auc: 0.751477\n",
      "[390]\tvalid_0's auc: 0.752539\n",
      "[400]\tvalid_0's auc: 0.753591\n",
      "[410]\tvalid_0's auc: 0.754599\n",
      "[420]\tvalid_0's auc: 0.755466\n",
      "[430]\tvalid_0's auc: 0.756299\n",
      "[440]\tvalid_0's auc: 0.757042\n",
      "[450]\tvalid_0's auc: 0.757681\n",
      "[460]\tvalid_0's auc: 0.758305\n",
      "[470]\tvalid_0's auc: 0.758906\n",
      "[480]\tvalid_0's auc: 0.759524\n",
      "[490]\tvalid_0's auc: 0.760105\n",
      "[500]\tvalid_0's auc: 0.760707\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's auc: 0.760707\n",
      "Predicting..\n",
      "Predicting..\n",
      "Training..\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.703156\n",
      "[20]\tvalid_0's auc: 0.708028\n",
      "[30]\tvalid_0's auc: 0.709577\n",
      "[40]\tvalid_0's auc: 0.710849\n",
      "[50]\tvalid_0's auc: 0.712706\n",
      "[60]\tvalid_0's auc: 0.714549\n",
      "[70]\tvalid_0's auc: 0.715914\n",
      "[80]\tvalid_0's auc: 0.717218\n",
      "[90]\tvalid_0's auc: 0.718286\n",
      "[100]\tvalid_0's auc: 0.71904\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.71904\n",
      "Predicting..\n",
      "Predicting..\n",
      "Training..\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.712578\n",
      "[20]\tvalid_0's auc: 0.717898\n",
      "[30]\tvalid_0's auc: 0.720689\n",
      "[40]\tvalid_0's auc: 0.721694\n",
      "[50]\tvalid_0's auc: 0.723191\n",
      "[60]\tvalid_0's auc: 0.724369\n",
      "[70]\tvalid_0's auc: 0.725417\n",
      "[80]\tvalid_0's auc: 0.726026\n",
      "[90]\tvalid_0's auc: 0.726876\n",
      "[100]\tvalid_0's auc: 0.72746\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.72746\n",
      "Predicting..\n",
      "Predicting..\n",
      "Training..\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.709449\n",
      "[20]\tvalid_0's auc: 0.712938\n",
      "[30]\tvalid_0's auc: 0.716073\n",
      "[40]\tvalid_0's auc: 0.718835\n",
      "[50]\tvalid_0's auc: 0.720581\n",
      "[60]\tvalid_0's auc: 0.721957\n",
      "[70]\tvalid_0's auc: 0.722865\n",
      "[80]\tvalid_0's auc: 0.724009\n",
      "[90]\tvalid_0's auc: 0.725\n",
      "[100]\tvalid_0's auc: 0.725814\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.725814\n",
      "Predicting..\n",
      "Predicting..\n",
      "Training..\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[10]\tvalid_0's auc: 0.707857\n",
      "[20]\tvalid_0's auc: 0.709522\n",
      "[30]\tvalid_0's auc: 0.711889\n",
      "[40]\tvalid_0's auc: 0.713136\n",
      "[50]\tvalid_0's auc: 0.715156\n",
      "[60]\tvalid_0's auc: 0.717055\n",
      "[70]\tvalid_0's auc: 0.718213\n",
      "[80]\tvalid_0's auc: 0.71928\n",
      "[90]\tvalid_0's auc: 0.720352\n",
      "[100]\tvalid_0's auc: 0.721227\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.721227\n",
      "Predicting..\n",
      "Predicting..\n"
     ]
    }
   ],
   "source": [
    "lg = lightgbmWrapper(seed=SEED, params=lgbm_params)\n",
    "\n",
    "print('lg..')\n",
    "lg_oof_train, lg_oof_test = get_oof_xgb(lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = np.array(lg_oof_test)\n",
    "np.save('lg', lg)\n",
    "\n",
    "subm['TARGET'] = lg\n",
    "subm.to_csv('light.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "354px",
    "left": "1568px",
    "right": "20px",
    "top": "117px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
