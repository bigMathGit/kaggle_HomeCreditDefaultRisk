{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "print('Importing data...')\n",
    "path = 'Data/'\n",
    "data = pd.read_csv(path+'application_train.csv')\n",
    "test = pd.read_csv(path+'application_test.csv')\n",
    "prev = pd.read_csv(path+'previous_application.csv')\n",
    "buro = pd.read_csv(path+'bureau.csv')\n",
    "buro_balance = pd.read_csv(path+'bureau_balance.csv')\n",
    "credit_card  = pd.read_csv(path+'credit_card_balance.csv')\n",
    "POS_CASH  = pd.read_csv(path+'POS_CASH_balance.csv')\n",
    "payments = pd.read_csv(path+'installments_payments.csv')\n",
    "lgbm_submission = pd.read_csv(path+'sample_submission.csv')\n",
    "\n",
    "#Separate target variable\n",
    "y = data['TARGET']\n",
    "del data['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding of categorical features in data and test sets\n",
    "categorical_features = [col for col in data.columns if data[col].dtype == 'object']\n",
    "\n",
    "one_hot_df = pd.concat([data,test])\n",
    "one_hot_df = pd.get_dummies(one_hot_df, columns=categorical_features)\n",
    "\n",
    "data = one_hot_df.iloc[:data.shape[0],:]\n",
    "test = one_hot_df.iloc[data.shape[0]:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing previous_application...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing previous_application\n",
    "print('Pre-processing previous_application...')\n",
    "#One-hot encoding of categorical features in previous application data set\n",
    "prev_cat_features = [pcol for pcol in prev.columns if prev[pcol].dtype == 'object']\n",
    "prev = pd.get_dummies(prev, columns=prev_cat_features)\n",
    "avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "cnt_prev = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "avg_prev['nb_app'] = cnt_prev['SK_ID_PREV']\n",
    "del avg_prev['SK_ID_PREV']\n",
    "\n",
    "max_prev = prev.groupby('SK_ID_CURR').max()\n",
    "min_prev = prev.groupby('SK_ID_CURR').min()\n",
    "sum_prev = prev.groupby('SK_ID_CURR').sum()\n",
    "\n",
    "max_prev.columns = max_prev.columns + '_max'\n",
    "max_prev.rename(columns={'SK_ID_CURR_max': 'SK_ID_CURR'}, inplace=True)\n",
    "\n",
    "min_prev.columns = min_prev.columns + '_min'\n",
    "min_prev.rename(columns={'SK_ID_CURR_min': 'SK_ID_CURR'}, inplace=True)\n",
    "\n",
    "sum_prev.columns = sum_prev.columns + '_sum'\n",
    "sum_prev.rename(columns={'SK_ID_CURR_sum': 'SK_ID_CURR'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing buro_balance...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing buro_balance\n",
    "print('Pre-processing buro_balance...')\n",
    "buro_grouped_size = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].size()\n",
    "buro_grouped_max = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].max()\n",
    "buro_grouped_min = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].min()\n",
    "buro_grouped_median = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].median()\n",
    "\n",
    "buro_counts = buro_balance.groupby('SK_ID_BUREAU')['STATUS'].value_counts(normalize = False)\n",
    "buro_counts_unstacked = buro_counts.unstack('STATUS')\n",
    "buro_counts_unstacked.columns = ['STATUS_0', 'STATUS_1','STATUS_2','STATUS_3','STATUS_4','STATUS_5','STATUS_C','STATUS_X',]\n",
    "buro_counts_unstacked['MONTHS_COUNT'] = buro_grouped_size\n",
    "buro_counts_unstacked['MONTHS_MIN'] = buro_grouped_min\n",
    "buro_counts_unstacked['MONTHS_MAX'] = buro_grouped_max\n",
    "buro_counts_unstacked['MONTHS_MEDIAN'] = buro_grouped_median\n",
    "buro = buro.join(buro_counts_unstacked, how='left', on='SK_ID_BUREAU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing buro...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing buro\n",
    "print('Pre-processing buro...')\n",
    "\n",
    "buro_active = buro.groupby('SK_ID_CURR')['CREDIT_ACTIVE'].value_counts(normalize = False)\n",
    "buro_active_unstacked = buro_active.unstack('CREDIT_ACTIVE')\n",
    "buro_active_unstacked.columns = ['CREDIT_ACTIVE_A','CREDIT_ACTIVE_B','CREDIT_ACTIVE_C','CREDIT_ACTIVE_S']\n",
    "\n",
    "buro_credittype = buro.groupby('SK_ID_CURR')['CREDIT_TYPE'].value_counts(normalize = False)\n",
    "buro_credittype_unstacked = buro_credittype.unstack('CREDIT_TYPE')\n",
    "buro_credittype_unstacked.columns = 'CREDIT_TYPE_' + buro_credittype_unstacked.columns\n",
    "\n",
    "del buro['CREDIT_ACTIVE']\n",
    "del buro['CREDIT_TYPE']\n",
    "\n",
    "#One-hot encoding of categorical features in buro data set\n",
    "buro_cat_features = [bcol for bcol in buro.columns if buro[bcol].dtype == 'object']\n",
    "buro = pd.get_dummies(buro, columns=buro_cat_features)\n",
    "\n",
    "avg_buro = buro.groupby('SK_ID_CURR').mean()\n",
    "avg_buro['buro_count'] = buro[['SK_ID_BUREAU', 'SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\n",
    "del avg_buro['SK_ID_BUREAU']\n",
    "\n",
    "max_buro = buro.groupby('SK_ID_CURR').max()\n",
    "max_buro.columns = max_buro.columns + '_max'\n",
    "max_buro.rename(columns={'SK_ID_CURR_max': 'SK_ID_CURR'}, inplace=True)\n",
    "del max_buro['SK_ID_BUREAU_max']\n",
    "min_buro = buro.groupby('SK_ID_CURR').min()\n",
    "min_buro.columns = min_buro.columns + '_min'\n",
    "min_buro.rename(columns={'SK_ID_CURR_min': 'SK_ID_CURR'}, inplace=True)\n",
    "del min_buro['SK_ID_BUREAU_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing POS_CASH...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing POS_CASH\n",
    "print('Pre-processing POS_CASH...')\n",
    "le = LabelEncoder()\n",
    "POS_CASH['NAME_CONTRACT_STATUS'] = le.fit_transform(POS_CASH['NAME_CONTRACT_STATUS'].astype(str))\n",
    "nunique_status = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\n",
    "nunique_status2 = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').max()\n",
    "POS_CASH['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\n",
    "POS_CASH['NUNIQUE_STATUS2'] = nunique_status2['NAME_CONTRACT_STATUS']\n",
    "\n",
    "max_POS_CASH = POS_CASH[['SK_ID_CURR', 'MONTHS_BALANCE', 'CNT_INSTALMENT', 'SK_DPD', 'SK_DPD_DEF']].groupby('SK_ID_CURR').max()\n",
    "min_POS_CASH = POS_CASH[['SK_ID_CURR', 'MONTHS_BALANCE', 'CNT_INSTALMENT', 'SK_DPD', 'SK_DPD_DEF']].groupby('SK_ID_CURR').min()\n",
    "\n",
    "max_POS_CASH.columns = max_POS_CASH.columns + '_max'\n",
    "max_POS_CASH.rename(columns={'SK_ID_CURR_max': 'SK_ID_CURR'}, inplace=True)\n",
    "min_POS_CASH.columns = min_POS_CASH.columns + '_min'\n",
    "min_POS_CASH.rename(columns={'SK_ID_CURR_min': 'SK_ID_CURR'}, inplace=True)\n",
    "POS_CASH.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing credit_card...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing credit_card\n",
    "print('Pre-processing credit_card...')\n",
    "credit_card['NAME_CONTRACT_STATUS'] = le.fit_transform(credit_card['NAME_CONTRACT_STATUS'].astype(str))\n",
    "nunique_status = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\n",
    "nunique_status2 = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').max()\n",
    "credit_card['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\n",
    "credit_card['NUNIQUE_STATUS2'] = nunique_status2['NAME_CONTRACT_STATUS']\n",
    "credit_card.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)\n",
    "\n",
    "max_credit_card = credit_card.groupby('SK_ID_CURR').max()\n",
    "min_creidt_card = credit_card.groupby('SK_ID_CURR').min()\n",
    "\n",
    "max_credit_card.columns = max_credit_card.columns + '_max'\n",
    "max_credit_card.rename(columns={'SK_ID_CURR_max': 'SK_ID_CURR'}, inplace=True)\n",
    "min_creidt_card.columns = min_creidt_card.columns + '_min'\n",
    "min_creidt_card.rename(columns={'SK_ID_CURR_min': 'SK_ID_CURR'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing payments...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing payments\n",
    "print('Pre-processing payments...')\n",
    "avg_payments = payments.groupby('SK_ID_CURR').mean()\n",
    "avg_payments2 = payments.groupby('SK_ID_CURR').max()\n",
    "avg_payments3 = payments.groupby('SK_ID_CURR').min()\n",
    "del avg_payments['SK_ID_PREV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining databases...\n"
     ]
    }
   ],
   "source": [
    "#Join data bases\n",
    "print('Joining databases...')\n",
    "data = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=max_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=max_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=min_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=min_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=sum_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=sum_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=max_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=max_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=min_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=min_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=buro_active_unstacked.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=buro_active_unstacked.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=buro_credittype_unstacked.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=buro_credittype_unstacked.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right = max_POS_CASH.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right = max_POS_CASH.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right = min_POS_CASH.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right = min_POS_CASH.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "data = data.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right = max_credit_card.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right = max_credit_card.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right = min_creidt_card.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right = min_creidt_card.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "data = data.merge(right=avg_payments.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_payments.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=avg_payments2.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_payments2.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=avg_payments3.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_payments3.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SK_ID_CURR', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT_x',\n",
       "       'AMT_ANNUITY_x', 'AMT_GOODS_PRICE_x', 'REGION_POPULATION_RELATIVE',\n",
       "       'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION',\n",
       "       ...\n",
       "       'DAYS_ENTRY_PAYMENT_y', 'AMT_INSTALMENT_y', 'AMT_PAYMENT_y',\n",
       "       'SK_ID_PREV_y', 'NUM_INSTALMENT_VERSION', 'NUM_INSTALMENT_NUMBER',\n",
       "       'DAYS_INSTALMENT', 'DAYS_ENTRY_PAYMENT', 'AMT_INSTALMENT',\n",
       "       'AMT_PAYMENT'],\n",
       "      dtype='object', length=1102)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X1'] = data['AMT_CREDIT_x']/data['AMT_INCOME_TOTAL']\n",
    "data['X2'] = data['AMT_CREDIT_y']/data['AMT_INCOME_TOTAL']\n",
    "data['X3'] = data['AMT_ANNUITY']/data['AMT_INCOME_TOTAL']\n",
    "data['X4'] = data['AMT_ANNUITY_x']/data['AMT_INCOME_TOTAL']\n",
    "data['X5'] = data['AMT_ANNUITY_y']/data['AMT_INCOME_TOTAL']\n",
    "data['X6'] = data['AMT_CREDIT_x']/data['AMT_ANNUITY_x']\n",
    "data['X7'] = data['AMT_CREDIT_y']/data['AMT_ANNUITY_x']\n",
    "data['X8'] = data['AMT_CREDIT_x']/data['AMT_ANNUITY_y']\n",
    "data['X9'] = data['AMT_CREDIT_y']/data['AMT_ANNUITY_y']\n",
    "data['X10'] = data['AMT_CREDIT_SUM']/data['AMT_CREDIT_x']\n",
    "data['x11'] = data['AMT_CREDIT_SUM']/data['AMT_CREDIT_y']\n",
    "data['X12'] = data['AMT_CREDIT_SUM_DEBT']/data['AMT_CREDIT_x']\n",
    "data['X13'] = data['AMT_CREDIT_SUM_DEBT']/data['AMT_CREDIT_y']\n",
    "data['X14'] = data['AMT_CREDIT_SUM_DEBT']/data['AMT_INCOME_TOTAL']\n",
    "data['X15'] = data['AMT_ANNUITY_x']/data['AMT_CREDIT_y']\n",
    "data['X16'] = data['AMT_ANNUITY_y']/data['AMT_CREDIT_x']\n",
    "data['X17'] = data['AMT_ANNUITY_x']/data['AMT_CREDIT_x']\n",
    "data['X18'] = data['AMT_ANNUITY_y']/data['AMT_CREDIT_y']\n",
    "data['X19'] = data['AMT_CREDIT_x']/data['AMT_GOODS_PRICE_x']\n",
    "data['DAYS_EMPLOYED_PERC'] = data['DAYS_EMPLOYED'] / data['DAYS_BIRTH']\n",
    "data['INCOME_CREDIT_PERC_x'] = data['AMT_INCOME_TOTAL'] / data['AMT_CREDIT_x']\n",
    "data['INCOME_CREDIT_PERC_y'] = data['AMT_INCOME_TOTAL'] / data['AMT_CREDIT_y']\n",
    "data['INCOME_PER_PERSON'] = data['AMT_INCOME_TOTAL'] / data['CNT_FAM_MEMBERS']\n",
    "data['ANNUITY_INCOME_PERC_x'] = data['AMT_ANNUITY_x'] / data['AMT_INCOME_TOTAL']\n",
    "data['ANNUITY_INCOME_PERC_y'] = data['AMT_ANNUITY_y'] / data['AMT_INCOME_TOTAL']\n",
    "\n",
    "data['PAYMENT_PERC'] = data['AMT_PAYMENT'] / data['AMT_INSTALMENT']\n",
    "data['PAYMENT_DIFF'] = data['AMT_INSTALMENT'] - data['AMT_PAYMENT']\n",
    "# Days past due and days before due (no negative values)\n",
    "data['DPD'] = data['DAYS_ENTRY_PAYMENT'] - data['AMT_INSTALMENT']\n",
    "data['DBD'] = data['AMT_INSTALMENT'] - data['DAYS_ENTRY_PAYMENT']\n",
    "data['DPD'] = data['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "data['DBD'] = data['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "test['X1'] = test['AMT_CREDIT_x']/test['AMT_INCOME_TOTAL']\n",
    "test['X2'] = test['AMT_CREDIT_y']/test['AMT_INCOME_TOTAL']\n",
    "test['X3'] = test['AMT_ANNUITY']/test['AMT_INCOME_TOTAL']\n",
    "test['X4'] = test['AMT_ANNUITY_x']/test['AMT_INCOME_TOTAL']\n",
    "test['X5'] = test['AMT_ANNUITY_y']/test['AMT_INCOME_TOTAL']\n",
    "test['X6'] = test['AMT_CREDIT_x']/test['AMT_ANNUITY_x']\n",
    "test['X7'] = test['AMT_CREDIT_y']/test['AMT_ANNUITY_x']\n",
    "test['X8'] = test['AMT_CREDIT_x']/test['AMT_ANNUITY_y']\n",
    "test['X9'] = test['AMT_CREDIT_y']/test['AMT_ANNUITY_y']\n",
    "test['X10'] = test['AMT_CREDIT_SUM']/test['AMT_CREDIT_x']\n",
    "test['x11'] = test['AMT_CREDIT_SUM']/test['AMT_CREDIT_y']\n",
    "test['X12'] = test['AMT_CREDIT_SUM_DEBT']/test['AMT_CREDIT_x']\n",
    "test['X13'] = test['AMT_CREDIT_SUM_DEBT']/test['AMT_CREDIT_y']\n",
    "test['X14'] = test['AMT_CREDIT_SUM_DEBT']/test['AMT_INCOME_TOTAL']\n",
    "test['X15'] = test['AMT_ANNUITY_x']/test['AMT_CREDIT_y']\n",
    "test['X16'] = test['AMT_ANNUITY_y']/test['AMT_CREDIT_x']\n",
    "test['X17'] = test['AMT_ANNUITY_x']/test['AMT_CREDIT_x']\n",
    "test['X18'] = test['AMT_ANNUITY_y']/test['AMT_CREDIT_y']\n",
    "test['X19'] = test['AMT_CREDIT_x']/test['AMT_GOODS_PRICE_x']\n",
    "test['DAYS_EMPLOYED_PERC'] = test['DAYS_EMPLOYED'] / test['DAYS_BIRTH']\n",
    "test['INCOME_CREDIT_PERC_x'] = test['AMT_INCOME_TOTAL'] / test['AMT_CREDIT_x']\n",
    "test['INCOME_CREDIT_PERC_y'] = test['AMT_INCOME_TOTAL'] / test['AMT_CREDIT_y']\n",
    "test['INCOME_PER_PERSON'] = test['AMT_INCOME_TOTAL'] / test['CNT_FAM_MEMBERS']\n",
    "test['ANNUITY_INCOME_PERC_x'] = test['AMT_ANNUITY_x'] / test['AMT_INCOME_TOTAL']\n",
    "test['ANNUITY_INCOME_PERC_y'] = test['AMT_ANNUITY_y'] / test['AMT_INCOME_TOTAL']\n",
    "\n",
    "test['PAYMENT_PERC'] = test['AMT_PAYMENT'] / test['AMT_INSTALMENT']\n",
    "test['PAYMENT_DIFF'] = test['AMT_INSTALMENT'] - test['AMT_PAYMENT']\n",
    "# Days past due and days before due (no negative values)\n",
    "test['DPD'] = test['DAYS_ENTRY_PAYMENT'] - test['AMT_INSTALMENT']\n",
    "test['DBD'] = test['AMT_INSTALMENT'] - test['DAYS_ENTRY_PAYMENT']\n",
    "test['DPD'] = test['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "test['DBD'] = test['DBD'].apply(lambda x: x if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing features with more than 80% missing...\n"
     ]
    }
   ],
   "source": [
    "#Remove features with many missing values\n",
    "print('Removing features with more than 80% missing...')\n",
    "test = test[test.columns[data.isnull().mean() < 0.85]]\n",
    "data = data[data.columns[data.isnull().mean() < 0.85]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete customer Id\n",
    "del data['SK_ID_CURR']\n",
    "del test['SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 40 rounds.\n",
      "[10]\ttraining's auc: 0.725404\tvalid_1's auc: 0.712794\n",
      "[20]\ttraining's auc: 0.746718\tvalid_1's auc: 0.73\n",
      "[30]\ttraining's auc: 0.754183\tvalid_1's auc: 0.734602\n",
      "[40]\ttraining's auc: 0.766343\tvalid_1's auc: 0.742202\n",
      "[50]\ttraining's auc: 0.776276\tvalid_1's auc: 0.747844\n",
      "[60]\ttraining's auc: 0.785086\tvalid_1's auc: 0.75306\n",
      "[70]\ttraining's auc: 0.79336\tvalid_1's auc: 0.758087\n",
      "[80]\ttraining's auc: 0.801829\tvalid_1's auc: 0.763274\n",
      "[90]\ttraining's auc: 0.809035\tvalid_1's auc: 0.766972\n",
      "[100]\ttraining's auc: 0.815458\tvalid_1's auc: 0.769942\n",
      "[110]\ttraining's auc: 0.821223\tvalid_1's auc: 0.772325\n",
      "[120]\ttraining's auc: 0.826464\tvalid_1's auc: 0.774116\n",
      "[130]\ttraining's auc: 0.831523\tvalid_1's auc: 0.775795\n",
      "[140]\ttraining's auc: 0.836343\tvalid_1's auc: 0.777105\n",
      "[150]\ttraining's auc: 0.841067\tvalid_1's auc: 0.778327\n",
      "[160]\ttraining's auc: 0.845426\tvalid_1's auc: 0.779453\n",
      "[170]\ttraining's auc: 0.849717\tvalid_1's auc: 0.780071\n",
      "[180]\ttraining's auc: 0.853807\tvalid_1's auc: 0.780806\n",
      "[190]\ttraining's auc: 0.857659\tvalid_1's auc: 0.781338\n",
      "[200]\ttraining's auc: 0.861365\tvalid_1's auc: 0.782048\n",
      "[210]\ttraining's auc: 0.865155\tvalid_1's auc: 0.782689\n",
      "[220]\ttraining's auc: 0.868661\tvalid_1's auc: 0.782842\n",
      "[230]\ttraining's auc: 0.872048\tvalid_1's auc: 0.783055\n",
      "[240]\ttraining's auc: 0.875661\tvalid_1's auc: 0.783407\n",
      "[250]\ttraining's auc: 0.878732\tvalid_1's auc: 0.783619\n",
      "[260]\ttraining's auc: 0.881745\tvalid_1's auc: 0.783814\n",
      "[270]\ttraining's auc: 0.884536\tvalid_1's auc: 0.783999\n",
      "[280]\ttraining's auc: 0.887537\tvalid_1's auc: 0.784178\n",
      "[290]\ttraining's auc: 0.890162\tvalid_1's auc: 0.784212\n",
      "[300]\ttraining's auc: 0.892933\tvalid_1's auc: 0.784303\n",
      "[310]\ttraining's auc: 0.895494\tvalid_1's auc: 0.784338\n",
      "[320]\ttraining's auc: 0.898102\tvalid_1's auc: 0.784425\n",
      "[330]\ttraining's auc: 0.900429\tvalid_1's auc: 0.78443\n",
      "[340]\ttraining's auc: 0.90261\tvalid_1's auc: 0.784462\n",
      "[350]\ttraining's auc: 0.904976\tvalid_1's auc: 0.78445\n",
      "[360]\ttraining's auc: 0.907223\tvalid_1's auc: 0.784533\n",
      "[370]\ttraining's auc: 0.909406\tvalid_1's auc: 0.784586\n",
      "[380]\ttraining's auc: 0.911523\tvalid_1's auc: 0.78464\n",
      "[390]\ttraining's auc: 0.913599\tvalid_1's auc: 0.784573\n",
      "[400]\ttraining's auc: 0.915601\tvalid_1's auc: 0.784593\n",
      "[410]\ttraining's auc: 0.917422\tvalid_1's auc: 0.784646\n",
      "[420]\ttraining's auc: 0.919239\tvalid_1's auc: 0.784641\n",
      "[430]\ttraining's auc: 0.921058\tvalid_1's auc: 0.784602\n",
      "[440]\ttraining's auc: 0.922829\tvalid_1's auc: 0.784567\n",
      "[450]\ttraining's auc: 0.924543\tvalid_1's auc: 0.784494\n",
      "Early stopping, best iteration is:\n",
      "[417]\ttraining's auc: 0.918664\tvalid_1's auc: 0.784679\n",
      "Fold  1 AUC : 0.784679\n",
      "Training until validation scores don't improve for 40 rounds.\n",
      "[10]\ttraining's auc: 0.72954\tvalid_1's auc: 0.723891\n",
      "[20]\ttraining's auc: 0.744058\tvalid_1's auc: 0.732898\n",
      "[30]\ttraining's auc: 0.755583\tvalid_1's auc: 0.740133\n",
      "[40]\ttraining's auc: 0.76618\tvalid_1's auc: 0.746569\n",
      "[50]\ttraining's auc: 0.774054\tvalid_1's auc: 0.750576\n",
      "[60]\ttraining's auc: 0.784228\tvalid_1's auc: 0.7561\n",
      "[70]\ttraining's auc: 0.794182\tvalid_1's auc: 0.761255\n",
      "[80]\ttraining's auc: 0.80251\tvalid_1's auc: 0.765386\n",
      "[90]\ttraining's auc: 0.810316\tvalid_1's auc: 0.768955\n",
      "[100]\ttraining's auc: 0.81684\tvalid_1's auc: 0.771493\n",
      "[110]\ttraining's auc: 0.822962\tvalid_1's auc: 0.773342\n",
      "[120]\ttraining's auc: 0.828611\tvalid_1's auc: 0.774917\n",
      "[130]\ttraining's auc: 0.833681\tvalid_1's auc: 0.776353\n",
      "[140]\ttraining's auc: 0.838312\tvalid_1's auc: 0.777389\n",
      "[150]\ttraining's auc: 0.842842\tvalid_1's auc: 0.778256\n",
      "[160]\ttraining's auc: 0.846867\tvalid_1's auc: 0.779218\n",
      "[170]\ttraining's auc: 0.851032\tvalid_1's auc: 0.779841\n",
      "[180]\ttraining's auc: 0.854874\tvalid_1's auc: 0.780402\n",
      "[190]\ttraining's auc: 0.858628\tvalid_1's auc: 0.780863\n",
      "[200]\ttraining's auc: 0.862399\tvalid_1's auc: 0.781163\n",
      "[210]\ttraining's auc: 0.866036\tvalid_1's auc: 0.781547\n",
      "[220]\ttraining's auc: 0.869348\tvalid_1's auc: 0.781768\n",
      "[230]\ttraining's auc: 0.872661\tvalid_1's auc: 0.782101\n",
      "[240]\ttraining's auc: 0.875684\tvalid_1's auc: 0.782231\n",
      "[250]\ttraining's auc: 0.878757\tvalid_1's auc: 0.782301\n",
      "[260]\ttraining's auc: 0.881842\tvalid_1's auc: 0.782524\n",
      "[270]\ttraining's auc: 0.884712\tvalid_1's auc: 0.78258\n",
      "[280]\ttraining's auc: 0.887531\tvalid_1's auc: 0.782727\n",
      "[290]\ttraining's auc: 0.890257\tvalid_1's auc: 0.782795\n",
      "[300]\ttraining's auc: 0.892917\tvalid_1's auc: 0.782915\n",
      "[310]\ttraining's auc: 0.895423\tvalid_1's auc: 0.782882\n",
      "[320]\ttraining's auc: 0.897929\tvalid_1's auc: 0.782921\n",
      "[330]\ttraining's auc: 0.900448\tvalid_1's auc: 0.782872\n",
      "[340]\ttraining's auc: 0.902643\tvalid_1's auc: 0.782858\n",
      "[350]\ttraining's auc: 0.9049\tvalid_1's auc: 0.782773\n",
      "Early stopping, best iteration is:\n",
      "[318]\ttraining's auc: 0.897453\tvalid_1's auc: 0.782956\n",
      "Fold  2 AUC : 0.782956\n",
      "valid score: 0.7838\n",
      "training time: 7 mins\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time \n",
    "\n",
    "folds = StratifiedKFold(n_splits=2,random_state=6)\n",
    "oof_preds = np.zeros(data.shape[0])\n",
    "sub_preds = np.zeros(test.shape[0])\n",
    "\n",
    "start = time.time()\n",
    "valid_score = 0\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(data, y)):\n",
    "    trn_x, trn_y = data.iloc[trn_idx], y[trn_idx]\n",
    "    val_x, val_y = data.iloc[val_idx], y[val_idx]    \n",
    "    \n",
    "    train_data = lgb.Dataset(data=trn_x, label=trn_y)\n",
    "    valid_data = lgb.Dataset(data=val_x, label=val_y)\n",
    "    \n",
    "    params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : 10,\n",
    "          'objective': 'binary',\n",
    "          'nthread': 5,\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'auc'\n",
    "          }\n",
    "    \n",
    "    \n",
    "    lgb_es_model = lgb.train(params, train_data, 2500,valid_sets=[train_data, valid_data], early_stopping_rounds= 40, verbose_eval=10) \n",
    "    \n",
    "    oof_preds[val_idx] = lgb_es_model.predict(val_x, num_iteration=lgb_es_model.best_iteration)\n",
    "    sub_preds += lgb_es_model.predict(test, num_iteration=lgb_es_model.best_iteration) / folds.n_splits\n",
    "    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n",
    "    valid_score += roc_auc_score(val_y, oof_preds[val_idx])\n",
    "\n",
    "print('valid score:', str(round(valid_score/folds.n_splits,4)))\n",
    "\n",
    "end = time.time()\n",
    "print('training time:', str(round((end - start)/60)), 'mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test set and write to submit\n",
    "lgbm_submission.TARGET = sub_preds\n",
    "\n",
    "lgbm_submission.to_csv('lgbm_submission3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
