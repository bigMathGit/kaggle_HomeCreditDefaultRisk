{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pythonstudy/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my 1st OOF...never give up\n"
     ]
    }
   ],
   "source": [
    "print(\"my 1st OOF...never give up\")\n",
    "SEED = 7    # CR7 always..\n",
    "path = 'Data/'\n",
    "\n",
    "application_train = pd.read_csv(path + \"application_train.csv\")\n",
    "application_test = pd.read_csv(path + \"application_test.csv\")\n",
    "POS_CASH = pd.read_csv(path + 'POS_CASH_balance.csv')\n",
    "credit_card = pd.read_csv(path + 'credit_card_balance.csv')\n",
    "bureau = pd.read_csv(path + 'bureau.csv')\n",
    "bureau_balance = pd.read_csv(path + 'bureau_balance.csv')\n",
    "previous_app = pd.read_csv(path + 'previous_application.csv')\n",
    "installments = pd.read_csv(path + 'installments_payments.csv')\n",
    "subm = pd.read_csv(path+ \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting...\n"
     ]
    }
   ],
   "source": [
    "#POS_CASH = pd.read_csv(path+ 'POS_CASH_balance.csv')\n",
    "print(\"Converting...\")\n",
    "le = LabelEncoder()\n",
    "POS_CASH['NAME_CONTRACT_STATUS'] = le.fit_transform(POS_CASH['NAME_CONTRACT_STATUS'].astype(str))\n",
    "nunique_status = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\n",
    "count_status = POS_CASH.groupby('SK_ID_CURR').size()\n",
    "#print(count_status)\n",
    "#print(nunique_status)\n",
    "POS_CASH['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\n",
    "POS_CASH['pc_COUNT_STATUS'] = count_status\n",
    "POS_CASH.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)\n",
    "#print(POS_CASH[POS_CASH['NUNIQUE_STATUS'].notna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit_card = pd.read_csv(path+ 'credit_card_balance.csv')\n",
    "\n",
    "credit_card['NAME_CONTRACT_STATUS'] = le.fit_transform(credit_card['NAME_CONTRACT_STATUS'].astype(str))\n",
    "nunique_status = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\n",
    "count_status = credit_card.groupby('SK_ID_CURR').size()\n",
    "credit_card['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\n",
    "credit_card['cc_COUNT_STATUS'] = count_status\n",
    "credit_card.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance = pd.read_csv(path + 'bureau_balance.csv')\n",
    "bureau_balance['STATUS'] = le.fit_transform(bureau_balance['STATUS'].astype(str))\n",
    "unique_status =bureau_balance[['SK_ID_BUREAU','STATUS']].groupby('SK_ID_BUREAU').nunique()\n",
    "count_status = bureau_balance.groupby('SK_ID_BUREAU').size()\n",
    "bureau_balance['NUNIQUE_STATUS'] = unique_status['STATUS']\n",
    "bureau_balance['bb_COUNT_STATUS'] = count_status\n",
    "bureau_balance.drop(['STATUS'], axis=1, inplace=True)\n",
    "\n",
    "bureau = pd.read_csv(path+ 'bureau.csv')\n",
    "bureau = bureau.merge(bureau_balance.groupby('SK_ID_BUREAU').mean().reset_index(), \n",
    "                                                      how = 'left', on= 'SK_ID_BUREAU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_status = bureau.groupby('SK_ID_CURR').size()\n",
    "bureau['B_COUNT_STATUS'] = count_status\n",
    "bureau_cat_features = [f for f in bureau.columns if bureau[f].dtype == 'object']\n",
    "for f in bureau_cat_features:\n",
    "    bureau[f] = le.fit_transform(bureau[f].astype(str))\n",
    "    nunique = bureau[['SK_ID_CURR', f]].groupby('SK_ID_CURR').nunique()\n",
    "    bureau['NUNIQUE_'+f] = nunique[f]\n",
    "    bureau.drop([f], axis=1, inplace=True)\n",
    "bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_status = previous_app.groupby('SK_ID_CURR').size()\n",
    "previous_app['pa_COUNT_STATUS'] = count_status\n",
    "\n",
    "previous_app_cat_features = [f for f in previous_app.columns if previous_app[f].dtype == 'object']\n",
    "for f in previous_app_cat_features:\n",
    "    previous_app[f] = le.fit_transform(previous_app[f].astype(str))\n",
    "    nunique = previous_app[['SK_ID_CURR', f]].groupby('SK_ID_CURR').nunique()\n",
    "    previous_app['NUNIQUE_'+f] = nunique[f]\n",
    "    previous_app.drop([f], axis=1, inplace=True)\n",
    "previous_app.drop(['SK_ID_PREV'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_status = installments.groupby('SK_ID_CURR').size()\n",
    "installments['ip_COUNT_STATUS'] = count_status\n",
    "\n",
    "installments.drop(['SK_ID_PREV'], axis=1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging...\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging...\")\n",
    "data_train = application_train.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                             how='left', on='SK_ID_CURR')\n",
    "data_test = application_test.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                           how='left', on='SK_ID_CURR')\n",
    "\n",
    "data_train = data_train.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                         how='left', on='SK_ID_CURR')\n",
    "data_test = data_test.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                       how='left', on='SK_ID_CURR')\n",
    "                                                       \n",
    "data_train = data_train.merge(bureau.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                    how='left', on='SK_ID_CURR')\n",
    "data_test = data_test.merge(bureau.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                  how='left', on='SK_ID_CURR')\n",
    "                                                  \n",
    "data_train = data_train.merge(previous_app.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                          how='left', on='SK_ID_CURR')\n",
    "data_test = data_test.merge(previous_app.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                        how='left', on='SK_ID_CURR')\n",
    "\n",
    "data_train = data_train.merge(installments.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                         how = 'left', on='SK_ID_CURR')\n",
    "data_test =  data_test.merge(installments.groupby('SK_ID_CURR').mean().reset_index(),\n",
    "                                                        how = 'left', on='SK_ID_CURR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['X1'] = data_train['AMT_CREDIT_x']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X2'] = data_train['AMT_CREDIT_y']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X3'] = data_train['AMT_ANNUITY']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X4'] = data_train['AMT_ANNUITY_x']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X5'] = data_train['AMT_ANNUITY_y']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X6'] = data_train['AMT_CREDIT_x']/data_train['AMT_ANNUITY_x']\n",
    "data_train['X7'] = data_train['AMT_CREDIT_y']/data_train['AMT_ANNUITY_x']\n",
    "data_train['X8'] = data_train['AMT_CREDIT_x']/data_train['AMT_ANNUITY_y']\n",
    "data_train['X9'] = data_train['AMT_CREDIT_y']/data_train['AMT_ANNUITY_y']\n",
    "data_train['X10'] = data_train['AMT_CREDIT_SUM']/data_train['AMT_CREDIT_x']\n",
    "data_train['x11'] = data_train['AMT_CREDIT_SUM']/data_train['AMT_CREDIT_y']\n",
    "data_train['X12'] = data_train['AMT_CREDIT_SUM_DEBT']/data_train['AMT_CREDIT_x']\n",
    "data_train['X13'] = data_train['AMT_CREDIT_SUM_DEBT']/data_train['AMT_CREDIT_y']\n",
    "data_train['X14'] = data_train['AMT_CREDIT_SUM_DEBT']/data_train['AMT_INCOME_TOTAL']\n",
    "data_train['X15'] = data_train['AMT_ANNUITY_x']/data_train['AMT_CREDIT_y']\n",
    "data_train['X16'] = data_train['AMT_ANNUITY_y']/data_train['AMT_CREDIT_x']\n",
    "data_train['X17'] = data_train['AMT_ANNUITY_x']/data_train['AMT_CREDIT_x']\n",
    "data_train['X18'] = data_train['AMT_ANNUITY_y']/data_train['AMT_CREDIT_y']\n",
    "\n",
    "\n",
    "data_test['X1'] = data_test['AMT_CREDIT_x']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X2'] = data_test['AMT_CREDIT_y']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X3'] = data_test['AMT_ANNUITY']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X4'] = data_test['AMT_ANNUITY_x']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X5'] = data_test['AMT_ANNUITY_y']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X6'] = data_test['AMT_CREDIT_x']/data_test['AMT_ANNUITY_x']\n",
    "data_test['X7'] = data_test['AMT_CREDIT_y']/data_test['AMT_ANNUITY_x']\n",
    "data_test['X8'] = data_test['AMT_CREDIT_x']/data_test['AMT_ANNUITY_y']\n",
    "data_test['X9'] = data_test['AMT_CREDIT_y']/data_test['AMT_ANNUITY_y']\n",
    "data_test['X10'] = data_test['AMT_CREDIT_SUM']/data_test['AMT_CREDIT_x']\n",
    "data_test['x11'] = data_test['AMT_CREDIT_SUM']/data_test['AMT_CREDIT_y']\n",
    "data_test['X12'] = data_test['AMT_CREDIT_SUM_DEBT']/data_test['AMT_CREDIT_x']\n",
    "data_test['X13'] = data_test['AMT_CREDIT_SUM_DEBT']/data_test['AMT_CREDIT_y']\n",
    "data_test['X14'] = data_test['AMT_CREDIT_SUM_DEBT']/data_test['AMT_INCOME_TOTAL']\n",
    "data_test['X15'] = data_test['AMT_ANNUITY_x']/data_test['AMT_CREDIT_y']\n",
    "data_test['X16'] = data_test['AMT_ANNUITY_y']/data_test['AMT_CREDIT_x']\n",
    "data_test['X17'] = data_test['AMT_ANNUITY_x']/data_test['AMT_CREDIT_x']\n",
    "data_test['X18'] = data_test['AMT_ANNUITY_y']/data_test['AMT_CREDIT_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = data_train['TARGET']\n",
    "data_train.drop(['SK_ID_CURR', 'TARGET'], axis=1, inplace=True)\n",
    "data_test.drop(['SK_ID_CURR'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat features are: ['NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE']\n",
      "[ 0  1  2  3  9 10 11 12 13 26 30 38 84 85 87 88]\n"
     ]
    }
   ],
   "source": [
    "cat_features = [f for f in data_train.columns if data_train[f].dtype == 'object']\n",
    "def column_index(df, query_cols):\n",
    "    cols = df.columns.values  ## col name을 array형태로 return 합니다.\n",
    "    sidx = np.argsort(cols)\n",
    "    return sidx[np.searchsorted(cols, query_cols, sorter=sidx)]\n",
    "cat_features_inds = column_index(data_train, cat_features)    \n",
    "print(\"Cat features are: %s\" % [f for f in cat_features])\n",
    "print(cat_features_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 229)\n"
     ]
    }
   ],
   "source": [
    "for col in cat_features:\n",
    "    data_train[col] = le.fit_transform(data_train[col].astype(str))\n",
    "    data_test[col] = le.fit_transform(data_test[col].astype(str))\n",
    "    \n",
    "data_train.fillna(-1, inplace=True)\n",
    "data_test.fillna(-1, inplace=True)\n",
    "cols = data_train.columns\n",
    "\n",
    "ntrain = data_train.shape[0]\n",
    "ntest = data_test.shape[0]\n",
    "\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(data_train.shape[0], n_folds=5, shuffle=True, random_state=7)\n",
    "NFOLDS = 5\n",
    "x_train = np.array(data_train)\n",
    "x_test = np.array(data_test)\n",
    "y_train = target_train.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/mmueller/stacking-starter?scriptVersionId=390867/code\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=7, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        print(\"Training..\")\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        print(\"Predicting..\")\n",
    "        return self.clf.predict_proba(x)\n",
    "\n",
    "\n",
    "class XgbWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "        self.nrounds = params.pop('nrounds', 250)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        print(\"Training..\")\n",
    "        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n",
    "\n",
    "    def predict(self, x):\n",
    "        print(\"Predicting..\")\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))\n",
    "\n",
    "class lightgbmWrapper(object):\n",
    "    def __init__(self, seed=0, params=None):\n",
    "        self.param = params\n",
    "        self.seed = seed\n",
    "        \n",
    "    def train(self, x_tr, y_tr):\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_tr, y_tr, random_state=self.seed)\n",
    "        lgb_train = lgb.Dataset(data=x_train, label=y_train)\n",
    "        lgb_eval = lgb.Dataset(data=x_val, label=y_val)\n",
    "        print(\"Training..\")\n",
    "        self.lbgm = lgb.train(self.param, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=100, verbose_eval=10)\n",
    "\n",
    "    def predict(self, x):\n",
    "        print(\"Predicting..\")\n",
    "        return self.lbgm.predict(x)\n",
    "    \n",
    "\n",
    "def get_oof(clf):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)[:,1]  # or [:,0]\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)[:,1]  # or [:,0]\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "    \n",
    "def get_oof_xgb(clf):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)  # or [:,0]\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)  # or [:,0]\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_params = {\n",
    "    'n_jobs': 16,\n",
    "    'n_estimators': 100,\n",
    "    'max_features': 0.5,\n",
    "    'max_depth': 12,\n",
    "    'min_samples_leaf': 2,\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_jobs': 16,\n",
    "    'n_estimators': 100,\n",
    "    'max_features': 0.2,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.7,\n",
    "    'learning_rate': 0.075,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 7,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mae',\n",
    "    'nrounds': 350\n",
    "}\n",
    "\n",
    "cb_params = {\n",
    "    'iterations':1000,\n",
    "    'learning_rate':0.1,\n",
    "    'depth':6,\n",
    "    'l2_leaf_reg':40,\n",
    "    'bootstrap_type':'Bernoulli',\n",
    "    'subsample':0.7,\n",
    "    'scale_pos_weight':5,\n",
    "    'eval_metric':'AUC',\n",
    "    'metric_period':50,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':45,\n",
    "    'allow_writing_files':False    \n",
    "}\n",
    "\n",
    "lgbm_params = {'task': 'train', \n",
    "          'boosting_type': 'gbdt', \n",
    "          'objective': 'binary', \n",
    "          'metric': 'auc', \n",
    "          'learning_rate': 0.01, \n",
    "          'num_leaves': 32, \n",
    "          'num_iteration': 500,\n",
    "          'verbose': 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xg..\n",
      "Training..\n",
      "Predicting..\n",
      "Predicting..\n",
      "Training..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-055b139b59fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xg..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mxg_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxg_oof_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof_xgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"et..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0met_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met_oof_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0met\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-d53972d65659>\u001b[0m in \u001b[0;36mget_oof_xgb\u001b[0;34m(clf)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0moof_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or [:,0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-d53972d65659>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgbdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pythonstudy/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pythonstudy/lib/python3.5/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pythonstudy/lib/python3.5/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 895\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    896\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xg = XgbWrapper(seed=SEED, params=xgb_params)\n",
    "et = SklearnWrapper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "rf = SklearnWrapper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "cb = SklearnWrapper(clf=CatBoostClassifier, seed=SEED, params=cb_params)\n",
    "lg = lightgbmWrapper(seed=SEED, params=lgbm_params)\n",
    "\n",
    "\n",
    "print(\"xg..\")\n",
    "xg_oof_train, xg_oof_test = get_oof_xgb(xg)\n",
    "print(\"et..\")\n",
    "et_oof_train, et_oof_test = get_oof(et)\n",
    "print(\"rf..\")\n",
    "rf_oof_train, rf_oof_test = get_oof(rf)\n",
    "print(\"cb..\")\n",
    "cb_oof_train, cb_oof_test = get_oof(cb)\n",
    "print('lg..')\n",
    "lg_oof_train, lg_oof_test = get_oof_xgb(lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((xg_oof_train, et_oof_train, rf_oof_train, cb_oof_train,lg_oof_train), axis=1)\n",
    "x_test = np.concatenate((xg_oof_test, et_oof_test, rf_oof_test, cb_oof_test,lg_oof_test), axis=1)\n",
    "print(x_train[0:5,:])\n",
    "\n",
    "np.save('x_train', x_train)\n",
    "np.save('x_test', x_test)\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "print(dtrain)\n",
    "\n",
    "xgb_params = {\n",
    "    'seed': 0,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.6,\n",
    "    'learning_rate': 0.01,\n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 4,\n",
    "    'num_parallel_tree': 1,\n",
    "    'min_child_weight': 1,\n",
    "    'eval_metric': 'mae',\n",
    "}\n",
    "\n",
    "print(\"xgb cv..\")\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=500, nfold=4, seed=SEED, stratified=False,\n",
    "             early_stopping_rounds=25, verbose_eval=10, show_stdv=True)\n",
    "best_nrounds = res.shape[0] - 1\n",
    "\n",
    "\"\"\"\n",
    "print(\"\\nCatBoost...\")                                     \n",
    "cb_model = CatBoostClassifier(iterations=1000,\n",
    "                              learning_rate=0.1,\n",
    "                              depth=6,\n",
    "                              l2_leaf_reg=40,\n",
    "                              bootstrap_type='Bernoulli',\n",
    "                              subsample=0.7,\n",
    "                              scale_pos_weight=5,\n",
    "                              eval_metric='AUC',\n",
    "                              metric_period=50,\n",
    "                              od_type='Iter',\n",
    "                              od_wait=45,\n",
    "                              random_seed=17,\n",
    "                              allow_writing_files=False)\n",
    "\n",
    "cb_model.fit(X_train, y_train,\n",
    "             eval_set=(X_valid, y_valid),\n",
    "             cat_features=cat_features_inds,\n",
    "             use_best_model=True,\n",
    "             verbose=True)\n",
    "             \n",
    "print('AUC:', roc_auc_score(y_valid, cb_model.predict_proba(X_valid)[:,1]))\n",
    "y_preds = cb_model.predict_proba(data_test)[:,1]\n",
    "subm['TARGET'] = y_preds\n",
    "subm.to_csv('submission.csv', index=False)\n",
    "\"\"\"\n",
    "print(\"meta xgb train..\")\n",
    "gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "fi = gbdt.predict(dtest)\n",
    "fi = np.array(fi)\n",
    "np.save('fi', fi)\n",
    "\n",
    "subm['TARGET'] = fi\n",
    "subm.to_csv('stack3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "354px",
    "left": "1568px",
    "right": "20px",
    "top": "117px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
