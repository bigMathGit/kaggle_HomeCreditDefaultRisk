{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "print('Importing data...')\n",
    "path = 'Data/'\n",
    "data = pd.read_csv(path+'application_train.csv')\n",
    "test = pd.read_csv(path+'application_test.csv')\n",
    "prev = pd.read_csv(path+'previous_application.csv')\n",
    "buro = pd.read_csv(path+'bureau.csv')\n",
    "buro_balance = pd.read_csv(path+'bureau_balance.csv')\n",
    "credit_card  = pd.read_csv(path+'credit_card_balance.csv')\n",
    "POS_CASH  = pd.read_csv(path+'POS_CASH_balance.csv')\n",
    "payments = pd.read_csv(path+'installments_payments.csv')\n",
    "lgbm_submission = pd.read_csv(path+'sample_submission.csv')\n",
    "\n",
    "#Separate target variable\n",
    "y = data['TARGET']\n",
    "del data['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding of categorical features in data and test sets\n",
    "categorical_features = [col for col in data.columns if data[col].dtype == 'object']\n",
    "\n",
    "one_hot_df = pd.concat([data,test])\n",
    "one_hot_df = pd.get_dummies(one_hot_df, columns=categorical_features)\n",
    "\n",
    "data = one_hot_df.iloc[:data.shape[0],:]\n",
    "test = one_hot_df.iloc[data.shape[0]:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing previous_application...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing previous_application\n",
    "print('Pre-processing previous_application...')\n",
    "#One-hot encoding of categorical features in previous application data set\n",
    "prev_cat_features = [pcol for pcol in prev.columns if prev[pcol].dtype == 'object']\n",
    "prev = pd.get_dummies(prev, columns=prev_cat_features)\n",
    "avg_prev = prev.groupby('SK_ID_CURR').mean()\n",
    "cnt_prev = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "avg_prev['nb_app'] = cnt_prev['SK_ID_PREV']\n",
    "del avg_prev['SK_ID_PREV']\n",
    "\n",
    "max_prev = prev.groupby('SK_ID_CURR').max()\n",
    "min_prev = prev.groupby('SK_ID_CURR').min()\n",
    "sum_prev = prev.groupby('SK_ID_CURR').sum()\n",
    "\n",
    "max_prev.columns = max_prev.columns + '_max'\n",
    "max_prev.rename(columns={'SK_ID_CURR_max': 'SK_ID_CURR'}, inplace=True)\n",
    "\n",
    "min_prev.columns = min_prev.columns + '_min'\n",
    "min_prev.rename(columns={'SK_ID_CURR_min': 'SK_ID_CURR'}, inplace=True)\n",
    "\n",
    "sum_prev.columns = max_prev.columns + '_sum'\n",
    "sum_prev.rename(columns={'SK_ID_CURR_sum': 'SK_ID_CURR'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing buro_balance...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing buro_balance\n",
    "print('Pre-processing buro_balance...')\n",
    "buro_grouped_size = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].size()\n",
    "buro_grouped_max = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].max()\n",
    "buro_grouped_min = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].min()\n",
    "buro_grouped_median = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].median()\n",
    "\n",
    "buro_counts = buro_balance.groupby('SK_ID_BUREAU')['STATUS'].value_counts(normalize = False)\n",
    "buro_counts_unstacked = buro_counts.unstack('STATUS')\n",
    "buro_counts_unstacked.columns = ['STATUS_0', 'STATUS_1','STATUS_2','STATUS_3','STATUS_4','STATUS_5','STATUS_C','STATUS_X',]\n",
    "buro_counts_unstacked['MONTHS_COUNT'] = buro_grouped_size\n",
    "buro_counts_unstacked['MONTHS_MIN'] = buro_grouped_min\n",
    "buro_counts_unstacked['MONTHS_MAX'] = buro_grouped_max\n",
    "buro_counts_unstacked['MONTHS_MEDIAN'] = buro_grouped_median\n",
    "buro = buro.join(buro_counts_unstacked, how='left', on='SK_ID_BUREAU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing buro...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing buro\n",
    "print('Pre-processing buro...')\n",
    "\n",
    "buro_active = buro.groupby('SK_ID_CURR')['CREDIT_ACTIVE'].value_counts(normalize = False)\n",
    "buro_active_unstacked = buro_active.unstack('CREDIT_ACTIVE')\n",
    "buro_active_unstacked.columns = ['CREDIT_ACTIVE_A','CREDIT_ACTIVE_B','CREDIT_ACTIVE_C','CREDIT_ACTIVE_S']\n",
    "\n",
    "buro_credittype = buro.groupby('SK_ID_CURR')['CREDIT_TYPE'].value_counts(normalize = False)\n",
    "buro_credittype_unstacked = buro_credittype.unstack('CREDIT_TYPE')\n",
    "buro_credittype_unstacked.columns = 'CREDIT_TYPE_' + buro_credittype_unstacked.columns\n",
    "\n",
    "del buro['CREDIT_ACTIVE']\n",
    "del buro['CREDIT_TYPE']\n",
    "\n",
    "#One-hot encoding of categorical features in buro data set\n",
    "buro_cat_features = [bcol for bcol in buro.columns if buro[bcol].dtype == 'object']\n",
    "buro = pd.get_dummies(buro, columns=buro_cat_features)\n",
    "\n",
    "avg_buro = buro.groupby('SK_ID_CURR').mean()\n",
    "avg_buro['buro_count'] = buro[['SK_ID_BUREAU', 'SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\n",
    "del avg_buro['SK_ID_BUREAU']\n",
    "\n",
    "max_buro = buro.groupby('SK_ID_CURR').max()\n",
    "max_buro.columns = max_buro.columns + '_max'\n",
    "max_buro.rename(columns={'SK_ID_CURR_max': 'SK_ID_CURR'}, inplace=True)\n",
    "del max_buro['SK_ID_BUREAU_max']\n",
    "min_buro = buro.groupby('SK_ID_CURR').min()\n",
    "min_buro.columns = min_buro.columns + '_min'\n",
    "min_buro.rename(columns={'SK_ID_CURR_min': 'SK_ID_CURR'}, inplace=True)\n",
    "del min_buro['SK_ID_BUREAU_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing POS_CASH...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing POS_CASH\n",
    "print('Pre-processing POS_CASH...')\n",
    "le = LabelEncoder()\n",
    "POS_CASH['NAME_CONTRACT_STATUS'] = le.fit_transform(POS_CASH['NAME_CONTRACT_STATUS'].astype(str))\n",
    "nunique_status = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\n",
    "nunique_status2 = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').max()\n",
    "POS_CASH['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\n",
    "POS_CASH['NUNIQUE_STATUS2'] = nunique_status2['NAME_CONTRACT_STATUS']\n",
    "\n",
    "max_POS_CASH = POS_CASH[['SK_ID_CURR', 'MONTHS_BALANCE', 'CNT_INSTALMENT', 'SK_DPD', 'SK_DPD_DEF']].groupby('SK_ID_CURR').max()\n",
    "min_POS_CASH = POS_CASH[['SK_ID_CURR', 'MONTHS_BALANCE', 'CNT_INSTALMENT', 'SK_DPD', 'SK_DPD_DEF']].groupby('SK_ID_CURR').min()\n",
    "\n",
    "max_POS_CASH.columns = max_POS_CASH.columns + '_max'\n",
    "max_POS_CASH.rename(columns={'SK_ID_CURR_max': 'SK_ID_CURR'}, inplace=True)\n",
    "min_POS_CASH.columns = min_POS_CASH.columns + '_min'\n",
    "min_POS_CASH.rename(columns={'SK_ID_CURR_min': 'SK_ID_CURR'}, inplace=True)\n",
    "POS_CASH.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing credit_card...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing credit_card\n",
    "print('Pre-processing credit_card...')\n",
    "credit_card['NAME_CONTRACT_STATUS'] = le.fit_transform(credit_card['NAME_CONTRACT_STATUS'].astype(str))\n",
    "nunique_status = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\n",
    "nunique_status2 = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').max()\n",
    "credit_card['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\n",
    "credit_card['NUNIQUE_STATUS2'] = nunique_status2['NAME_CONTRACT_STATUS']\n",
    "credit_card.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)\n",
    "\n",
    "max_credit_card = credit_card.groupby('SK_ID_CURR').max()\n",
    "min_creidt_card = credit_card.groupby('SK_ID_CURR').min()\n",
    "\n",
    "max_credit_card.columns = max_credit_card.columns + '_max'\n",
    "max_credit_card.rename(columns={'SK_ID_CURR_max': 'SK_ID_CURR'}, inplace=True)\n",
    "min_creidt_card.columns = min_creidt_card.columns + '_min'\n",
    "min_creidt_card.rename(columns={'SK_ID_CURR_min': 'SK_ID_CURR'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing payments...\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing payments\n",
    "print('Pre-processing payments...')\n",
    "avg_payments = payments.groupby('SK_ID_CURR').mean()\n",
    "avg_payments2 = payments.groupby('SK_ID_CURR').max()\n",
    "avg_payments3 = payments.groupby('SK_ID_CURR').min()\n",
    "del avg_payments['SK_ID_PREV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining databases...\n"
     ]
    }
   ],
   "source": [
    "#Join data bases\n",
    "print('Joining databases...')\n",
    "data = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=max_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=max_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=min_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=min_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=sum_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=sum_prev.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=max_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=max_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=min_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=min_buro.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=buro_active_unstacked.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=buro_active_unstacked.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=buro_credittype_unstacked.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=buro_credittype_unstacked.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right = max_POS_CASH.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right = max_POS_CASH.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right = min_POS_CASH.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right = min_POS_CASH.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "data = data.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right = max_credit_card.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right = max_credit_card.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right = min_creidt_card.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right = min_creidt_card.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "\n",
    "data = data.merge(right=avg_payments.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_payments.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=avg_payments2.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_payments2.reset_index(), how='left', on='SK_ID_CURR')\n",
    "\n",
    "data = data.merge(right=avg_payments3.reset_index(), how='left', on='SK_ID_CURR')\n",
    "test = test.merge(right=avg_payments3.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SK_ID_CURR', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'AMT_CREDIT_x',\n",
       "       'AMT_ANNUITY_x', 'AMT_GOODS_PRICE_x', 'REGION_POPULATION_RELATIVE',\n",
       "       'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION',\n",
       "       ...\n",
       "       'DAYS_ENTRY_PAYMENT_y', 'AMT_INSTALMENT_y', 'AMT_PAYMENT_y',\n",
       "       'SK_ID_PREV_y', 'NUM_INSTALMENT_VERSION', 'NUM_INSTALMENT_NUMBER',\n",
       "       'DAYS_INSTALMENT', 'DAYS_ENTRY_PAYMENT', 'AMT_INSTALMENT',\n",
       "       'AMT_PAYMENT'],\n",
       "      dtype='object', length=1102)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['X1'] = data['AMT_CREDIT_x']/data['AMT_INCOME_TOTAL']\n",
    "data['X2'] = data['AMT_CREDIT_y']/data['AMT_INCOME_TOTAL']\n",
    "data['X3'] = data['AMT_ANNUITY']/data['AMT_INCOME_TOTAL']\n",
    "data['X4'] = data['AMT_ANNUITY_x']/data['AMT_INCOME_TOTAL']\n",
    "data['X5'] = data['AMT_ANNUITY_y']/data['AMT_INCOME_TOTAL']\n",
    "data['X6'] = data['AMT_CREDIT_x']/data['AMT_ANNUITY_x']\n",
    "data['X7'] = data['AMT_CREDIT_y']/data['AMT_ANNUITY_x']\n",
    "data['X8'] = data['AMT_CREDIT_x']/data['AMT_ANNUITY_y']\n",
    "data['X9'] = data['AMT_CREDIT_y']/data['AMT_ANNUITY_y']\n",
    "data['X10'] = data['AMT_CREDIT_SUM']/data['AMT_CREDIT_x']\n",
    "data['x11'] = data['AMT_CREDIT_SUM']/data['AMT_CREDIT_y']\n",
    "data['X12'] = data['AMT_CREDIT_SUM_DEBT']/data['AMT_CREDIT_x']\n",
    "data['X13'] = data['AMT_CREDIT_SUM_DEBT']/data['AMT_CREDIT_y']\n",
    "data['X14'] = data['AMT_CREDIT_SUM_DEBT']/data['AMT_INCOME_TOTAL']\n",
    "data['X15'] = data['AMT_ANNUITY_x']/data['AMT_CREDIT_y']\n",
    "data['X16'] = data['AMT_ANNUITY_y']/data['AMT_CREDIT_x']\n",
    "data['X17'] = data['AMT_ANNUITY_x']/data['AMT_CREDIT_x']\n",
    "data['X18'] = data['AMT_ANNUITY_y']/data['AMT_CREDIT_y']\n",
    "data['X19'] = data['AMT_CREDIT_x']/data['AMT_GOODS_PRICE_x']\n",
    "\n",
    "\n",
    "test['X1'] = test['AMT_CREDIT_x']/test['AMT_INCOME_TOTAL']\n",
    "test['X2'] = test['AMT_CREDIT_y']/test['AMT_INCOME_TOTAL']\n",
    "test['X3'] = test['AMT_ANNUITY']/test['AMT_INCOME_TOTAL']\n",
    "test['X4'] = test['AMT_ANNUITY_x']/test['AMT_INCOME_TOTAL']\n",
    "test['X5'] = test['AMT_ANNUITY_y']/test['AMT_INCOME_TOTAL']\n",
    "test['X6'] = test['AMT_CREDIT_x']/test['AMT_ANNUITY_x']\n",
    "test['X7'] = test['AMT_CREDIT_y']/test['AMT_ANNUITY_x']\n",
    "test['X8'] = test['AMT_CREDIT_x']/test['AMT_ANNUITY_y']\n",
    "test['X9'] = test['AMT_CREDIT_y']/test['AMT_ANNUITY_y']\n",
    "test['X10'] = test['AMT_CREDIT_SUM']/test['AMT_CREDIT_x']\n",
    "test['x11'] = test['AMT_CREDIT_SUM']/test['AMT_CREDIT_y']\n",
    "test['X12'] = test['AMT_CREDIT_SUM_DEBT']/test['AMT_CREDIT_x']\n",
    "test['X13'] = test['AMT_CREDIT_SUM_DEBT']/test['AMT_CREDIT_y']\n",
    "test['X14'] = test['AMT_CREDIT_SUM_DEBT']/test['AMT_INCOME_TOTAL']\n",
    "test['X15'] = test['AMT_ANNUITY_x']/test['AMT_CREDIT_y']\n",
    "test['X16'] = test['AMT_ANNUITY_y']/test['AMT_CREDIT_x']\n",
    "test['X17'] = test['AMT_ANNUITY_x']/test['AMT_CREDIT_x']\n",
    "test['X18'] = test['AMT_ANNUITY_y']/test['AMT_CREDIT_y']\n",
    "test['X19'] = test['AMT_CREDIT_x']/test['AMT_GOODS_PRICE_x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing features with more than 80% missing...\n"
     ]
    }
   ],
   "source": [
    "#Remove features with many missing values\n",
    "print('Removing features with more than 80% missing...')\n",
    "test = test[test.columns[data.isnull().mean() < 0.8]]\n",
    "data = data[data.columns[data.isnull().mean() < 0.8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete customer Id\n",
    "del data['SK_ID_CURR']\n",
    "del test['SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and validation set\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(data, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------Build LightGBM Model-----------------------\n",
    "train_data=lgb.Dataset(train_x,label=train_y)\n",
    "valid_data=lgb.Dataset(valid_x,label=valid_y)\n",
    "\n",
    "#Select Hyper-Parameters\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : 10,\n",
    "          'objective': 'binary',\n",
    "          'nthread': 5,\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'auc'\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Train model on selected parameters and number of iterations\n",
    "lgbm = lgb.train(params,\n",
    "                 train_data,\n",
    "                 2500,\n",
    "                 valid_sets=valid_data,\n",
    "                 early_stopping_rounds= 40,\n",
    "                 verbose_eval= 10\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test set and write to submit\n",
    "predictions_lgbm_prob = lgbm.predict(test)\n",
    "\n",
    "lgbm_submission.TARGET = predictions_lgbm_prob\n",
    "\n",
    "lgbm_submission.to_csv('lgbm_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Variable Importances\n",
    "lgb.plot_importance(lgbm, importance_type='split', figsize=(12, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test.columns[lgbm.feature_importance()<10]:\n",
    "    del test[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.columns[lgbm.feature_importance()<10]:\n",
    "    del data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(data, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------Build LightGBM Model-----------------------\n",
    "train_data=lgb.Dataset(train_x,label=train_y)\n",
    "valid_data=lgb.Dataset(valid_x,label=valid_y)\n",
    "\n",
    "#Select Hyper-Parameters\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : 10,\n",
    "          'objective': 'binary',\n",
    "          'nthread': 5,\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'auc'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model on selected parameters and number of iterations\n",
    "lgbm = lgb.train(params,\n",
    "                 train_data,\n",
    "                 2500,\n",
    "                 valid_sets=valid_data,\n",
    "                 early_stopping_rounds= 40,\n",
    "                 verbose_eval= 10\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test set and write to submit\n",
    "predictions_lgbm_prob = lgbm.predict(test)\n",
    "\n",
    "lgbm_submission.TARGET = predictions_lgbm_prob\n",
    "\n",
    "lgbm_submission.to_csv('lgbm_submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Variable Importances\n",
    "lgb.plot_importance(lgbm, importance_type='split', figsize=(12, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
